---
title: "Preliminary data analyses"
author: "Jussi Palom√§ki"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '4'
  # html_notebook:
  #   code_folding: show
  #   css: style.css
  #   theme: yeti
  #   toc: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
---

```{r error=TRUE}

suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(colorRamps))
suppressPackageStartupMessages(library(grDevices))
suppressPackageStartupMessages(library(lme4))
suppressPackageStartupMessages(library(effects))
suppressPackageStartupMessages(library(multcomp))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(lmerTest))
suppressPackageStartupMessages(library(ggpubr))
suppressPackageStartupMessages(library(MuMIn))
suppressPackageStartupMessages(library(lattice))
suppressPackageStartupMessages(library(robustlmm))
suppressPackageStartupMessages(library(sjPlot))
suppressPackageStartupMessages(library(grid))
suppressPackageStartupMessages(library(here))

# load data
indir <- file.path(here(), '..', 'data')
fss_learning <- read.csv(file.path(indir, "fss_learning.csv"))

#Create integer variable for participants (ID)
fss_learning <- fss_learning %>% mutate(ID=as.integer(participant))
fss_learning$ID <- as.factor(fss_learning$ID)

#Remove first row (cumrun == 1) for each participant. There can be no expectation of performance based on previous performance when participants play their first run.
fss_learning_sub <- subset(fss_learning, cumrun!=1)

#Fit models with both the first run removed and with all runs. Models are almost equal, with the one where first run is removed being slightly better
anova(lmer(flow ~ deviation + (deviation|participant), data=fss_learning_sub))
anova(lmer(flow ~ deviation + (deviation|participant), data=fss_learning))

#Investigate RQ: Does the effect of Deviation on Flow (main finding from previous paper) depend on level cumrun (how many runs the participant has played). Include all runs.

#Fit and view model (note: could also try random slope/intercept for deviation*cumrun):
fss_learning2_lmer <- lmer(flow ~ deviation*cumrun + (deviation|participant), data=fss_learning)
summary(fss_learning2_lmer)
anova(fss_learning2_lmer)
r.squaredGLMM(fss_learning2_lmer)

#Test model assumptions:
plot(fss_learning2_lmer)
plot(resid(fss_learning2_lmer), fss_learning$deviation)
plot(resid(fss_learning2_lmer), fss_learning$cumrun)
qqnorm(residuals(fss_learning2_lmer))
qqline(residuals(fss_learning2_lmer))
qqmath(ranef(fss_learning2_lmer, condVar = TRUE), strip = TRUE)

#Model assumptions suggest the model is "not great, not terrible" (i.e. "3.6").
#Fit model using robust lmm (COMMENTED OUT IN RMD DUE TO LONG SIMULATION TIME):
# fss_learning2_rlmer <- rlmer(flow ~ deviation*cumrun + (deviation|participant), data=fss_learning)
# summary(fss_learning2_rlmer)
# plot(fss_learning2_rlmer)

#Plot interaction:
plot_model(fss_learning2_lmer, type = "pred", terms = c("deviation", "cumrun"), mdrt.values = "meansd")

#Double check by plotting interaction by hand
#Calculate -1, mean, and +1 SD for cumrun
cumrun_minusSD <- mean(fss_learning$cumrun) - sd(fss_learning$cumrun)
cumrun_meanSD <- mean(fss_learning$cumrun)
cumrun_plusSD <- mean(fss_learning$cumrun) + sd(fss_learning$cumrun)

flow_interaction <- effect(c("deviation*cumrun"), fss_learning2_lmer,
                   xlevels=list(deviation=c(-0.2, 0, 0.2), #x-axis range
                                cumrun=c(cumrun_minusSD, cumrun_meanSD, cumrun_plusSD),
                                se=TRUE, confidence.level=.95, typical=mean))

flow_interaction <- as.data.frame(flow_interaction)
flow_interaction$cumrun <- as.factor(flow_interaction$cumrun)
ggplot(flow_interaction, aes(x=deviation, y=fit, colour=cumrun)) + geom_line()

#For participant-wise visualization, calculate slopes and intercepts of deviation at various levels of moderator (cumrun) for each participant. Fit LMs for each participant separately.
slope_intercept_dataframes = list()
fss_models <- list()
for (i in 1:length(unique(fss_learning$participant))) {
  temp_model = lm(flow ~ deviation*cumrun, data=subset(fss_learning, ID==i))
  flow_interaction <- effect(c("deviation*cumrun"), temp_model, 
                             xlevels=list(deviation=c(-0.2, 0, 0.2), #arbitrary levels, 0 to get intercept
                                          cumrun=c(cumrun_minusSD, cumrun_meanSD, cumrun_plusSD), #values are -1SD and +1SD
                                          se=TRUE, confidence.level=.95, typical=mean))
  flow_interaction <- as.data.frame(flow_interaction)
  flow_interaction$ID <- as.factor(i)
  flow_interaction$cumrun <- as.factor(flow_interaction$cumrun)
  flow_interaction <- flow_interaction %>% mutate(slope_minusSD = fit[3]-fit[1], #slope is the difference in FIT between 1 unit increments of deviation
                                                  slope_MEAN = fit[6]-fit[4],
                                                  slope_plusSD = fit[9]-fit[7],
                                                  intercept_minusSD = fit[2], #intercept is the value of fit when deviation = 0
                                                  intercept_MEAN = fit[5],
                                                  intercept_plusSD = fit[8])
  slope_intercept_dataframes = rbind(slope_intercept_dataframes, flow_interaction)
  fss_models[[i]] = temp_model
}

#Get p-values, sort from lowest to highest, and adjust using Bonferroni-Holm
fss_Pvalues <- list()
for (i in 1:18) {
  fss_Pvalues[[i]] <- anova(fss_models[[i]])$"Pr(>F)"[3]
}
fss_Pvalues <- as.data.frame(fss_Pvalues) 
colnames(fss_Pvalues) <- c(1:18) #18 = number of participants
fss_Pvalues <- sort(fss_Pvalues)
fss_Pvalues_holm <- p.adjust(fss_Pvalues, "holm")

#Get order of participants for plotting
part_levels <- as.integer(names(fss_Pvalues))

#Reorder levels in data according to p-value, from low (significant) to high (non-significant):
fss_learning$ID <- factor(fss_learning$ID, levels=part_levels)

#Start plotting results. First assign initial plot as flow_fig
flow_fig <- ggplot(fss_learning, aes(deviation, flow)) +
  geom_point(alpha=.2, size=1.5) +
  #geom_smooth(method = "lm", se = FALSE, fullrange=TRUE) + 
  facet_wrap(~ID) +
  geom_abline(data = slope_intercept_dataframes, aes(intercept=intercept_minusSD, slope=slope_minusSD, linetype="-1 SD (at 8.81 runs)"), size=0.8) + 
  geom_abline(data = slope_intercept_dataframes, aes(intercept=intercept_plusSD, slope=slope_plusSD, linetype="+1 SD (at 32.19 runs)"), size=0.8) + 
  labs(colour = NULL, linetype = "Cumulative runs", title = "Participant-wise models") + xlab("Deviation score") + ylab("Flow score") +
  theme_bw(base_size=14) + 
  theme(legend.position = c(x=.8, y=.1),
        legend.background = element_rect(fill="white"),
        legend.box.background = element_rect(colour = "black"),
        axis.text.x = element_text(size=8),
        panel.border = element_blank(), panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"),
        strip.background=element_rect(fill="grey"))

#Facet panel coloring function for plotting. Allows for changing desired alpha level (below which results are "statistically significant"). * LIGHT GREEN PANELS = interaction deviation*cumrum is statistically significant, given alpha level, after bonferroni-holm correction for 18 multiple comparisons. * DARK GREEN PANELS = interaction deviation*cumrum is statistically significant, given alpha level, without correcting for multiple comparisons. * GREY PANELS = deviation*cumrum is not statistically significant, given alpha level.
panel_colors <- function(alpha_level) {
  
  holm_adjusted_values <- rep("yellowgreen", sum(fss_Pvalues_holm < alpha_level))
  non_adjusted_values <- rep("yellow4", sum(fss_Pvalues < alpha_level)-sum(fss_Pvalues_holm < alpha_level)) #overlap between holm_adjusted removed!
  non_significant_values <- rep("grey69", 18-sum(fss_Pvalues < alpha_level)+sum(fss_Pvalues_holm < alpha_level)) #overlap between holm_adjusted and non_adjusted removed!
  all_values <- c(holm_adjusted_values,non_adjusted_values,non_significant_values)
  
  g <- ggplot_gtable(ggplot_build(flow_fig))
  stripr <- which(grepl('strip-t', g$layout$name))
  stripr <- rev(stripr[-c(4:5)])
  stripr <- c(rev(stripr[1:5]), rev(stripr[6:10]), rev(stripr[11:15]), rev(stripr[16:18]))
  fills <- all_values
  k <- 1
  for (i in stripr) {
    j <- which(grepl('rect', g$grobs[[i]]$grobs[[1]]$childrenOrder))
    g$grobs[[i]]$grobs[[1]]$children[[j]]$gp$fill <- fills[k]
    k <- k+1
  }
  grid.draw(g)
}

#Draw figure with alpha level at .05:
grid.newpage() #these allow knitr to draw everything (otherwise grid.draw() and ggplot() clash)
panel_colors(.05)

#Draw figure with alpha level at .1:
grid.newpage()
panel_colors(.1)